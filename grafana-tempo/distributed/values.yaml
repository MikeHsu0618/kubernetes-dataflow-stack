
fullnameOverride: tempo

# -- If true, Tempo will report anonymous usage data about the shape of a deployment to Grafana Labs
reportingEnabled: false

# Configuration for the ingester
ingester:
  replicas: 3

  autoscaling:
    # -- Enable autoscaling for the ingester. WARNING: Autoscaling ingesters can result in lost data. Only do this if you know what you're doing.
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    behavior: {}
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage:
  # -- Resource requests and limits for the ingester
  resources: {}

  config:
    # -- Number of copies of spans to store in the ingester ring
    replication_factor: 3
    # -- Amount of time a trace must be idle before flushing it to the wal.
    trace_idle_period: null
    # -- How often to sweep all tenants and move traces from live -> wal -> completed blocks.
    flush_check_period: null
    # -- Maximum size of a block before cutting it
    max_block_bytes: null
    # -- Maximum length of time before cutting a block
    max_block_duration: null
    # -- Duration to keep blocks in the ingester after they have been flushed
    complete_block_timeout: null

# Configuration for the distributor
distributor:
  replicas: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    behavior: {}
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage:
  # -- Resource requests and limits for the distributor
  resources: {}
  config:
    # -- Enable to log every received trace id to help debug ingestion
    # -- WARNING: Deprecated. Use log_received_spans instead.
    log_received_traces: null
    # -- Enable to log every received span to help debug ingestion or calculate span error distributions using the logs
    log_received_spans:
      enabled: false
      include_all_attributes: false
      filter_by_status_error: false
    # -- Disables write extension with inactive ingesters
    extend_writes: null


# Configuration for the compactor
compactor:
  # -- Number of replicas for the compactor
  replicas: 1
  # -- Resource requests and limits for the compactor
  resources: {}
  config:
    compaction:
      # -- Duration to keep blocks
      block_retention: 48h
      # Duration to keep blocks that have been compacted elsewhere
      compacted_block_retention: 1h
      # -- Blocks in this time window will be compacted together
      compaction_window: 1h
      # -- Amount of data to buffer from input blocks
      v2_in_buffer_bytes: 5242880
      # -- Flush data to backend when buffer is this large
      v2_out_buffer_bytes: 20971520
      # -- Maximum number of traces in a compacted block. WARNING: Deprecated. Use max_block_bytes instead.
      max_compaction_objects: 6000000
      # -- Maximum size of a compacted block in bytes
      max_block_bytes: 107374182400
      # -- Number of tenants to process in parallel during retention
      retention_concurrency: 10
      # -- Number of traces to buffer in memory during compaction
      v2_prefetch_traces_count: 1000
      # -- The maximum amount of time to spend compacting a single tenant before moving to the next
      max_time_per_tenant: 5m
      # -- The time between compaction cycles
      compaction_cycle: 30s
  service:
    # -- Annotations for compactor service
    annotations: {}

# Configuration for the querier
querier:
  replicas: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    behavior: {}
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage:
  resources: {}
  config:
    frontend_worker:
      # -- grpc client configuration
      grpc_client_config: {}
    trace_by_id:
      # -- Timeout for trace lookup requests
      query_timeout: 10s
    search:
      # -- Timeout for search requests
      query_timeout: 30s
      # -- If search_external_endpoints is set then the querier will primarily act as a proxy for whatever serverless backend you have configured. This setting allows the operator to have the querier prefer itself for a configurable number of subqueries.
      prefer_self: 10
      # -- If set to a non-zero value a second request will be issued at the provided duration. Recommended to be set to p99 of external search requests to reduce long tail latency.
      external_hedge_requests_at: 8s
      # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set.
      external_hedge_requests_up_to: 2
      # -- A list of external endpoints that the querier will use to offload backend search requests
      external_endpoints: []
      # -- The serverless backend to use. The default value of "" omits
      # -- credentials when querying the external backend.
      external_backend: ""
      # -- Google Cloud Run configuration. Will be used only if the value of
      # -- external_backend is "google_cloud_run".
      google_cloud_run: {}
    # -- This value controls the overall number of simultaneous subqueries that the querier will service at once. It does not distinguish between the types of queries.
    max_concurrent_queries: 20

  service:
    # -- Annotations for querier service
    annotations: {}
  # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: null

# Configuration for the query-frontend
queryFrontend:
  query:
    # -- Required for grafana version <7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch
    enabled: false
  replicas: 1

  config:
    # -- Number of times to retry a request sent to a querier
    max_retries: 2
    search:
      # -- The number of concurrent jobs to execute when searching the backend
      concurrent_jobs: 1000
      # -- The target number of bytes for each job to handle when performing a backend search
      target_bytes_per_job: 104857600
    # -- Trace by ID lookup configuration
    trace_by_id:
      # -- The number of shards to split a trace by id query into.
      query_shards: 50
      # -- If set to a non-zero value, a second request will be issued at the provided duration. Recommended to be set to p99 of search requests to reduce long-tail latency.
      hedge_requests_at: 2s
      # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set. Must be greater than 0.
      hedge_requests_up_to: 2
  autoscaling:
    # -- Enable autoscaling for the query-frontend
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    behavior: {}
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 60
  ingress:
    # -- Specifies whether an ingress for the Jaeger should be created
    enabled: false
  resources: {}

multitenancyEnabled: false

traces:
  jaeger:
    grpc:
      # -- Enable Tempo to ingest Jaeger GRPC traces
      enabled: true
      # -- Jaeger GRPC receiver config
      receiverConfig: {}
    thriftBinary:
      # -- Enable Tempo to ingest Jaeger Thrift Binary traces
      enabled: true
      # -- Jaeger Thrift Binary receiver config
      receiverConfig: {}
    thriftCompact:
      # -- Enable Tempo to ingest Jaeger Thrift Compact traces
      enabled: true
      # -- Jaeger Thrift Compact receiver config
      receiverConfig: {}
    thriftHttp:
      # -- Enable Tempo to ingest Jaeger Thrift HTTP traces
      enabled: true
      # -- Jaeger Thrift HTTP receiver config
      receiverConfig: {}
  zipkin:
    # -- Enable Tempo to ingest Zipkin traces
    enabled: true
    # -- Zipkin receiver config
    receiverConfig: {}
  otlp:
    http:
      # -- Enable Tempo to ingest Open Telemetry HTTP traces
      enabled: true
      # -- HTTP receiver advanced config
      receiverConfig: {}
    grpc:
      # -- Enable Tempo to ingest Open Telemetry GRPC traces
      enabled: true
      # -- GRPC receiver advanced config
      receiverConfig: {}
  opencensus:
    # -- Enable Tempo to ingest Open Census traces
    enabled: true
    # -- Open Census receiver config
    receiverConfig: {}


storage:
  trace:
    # Settings for the block storage backend and buckets.
    block:
      # -- The supported block versions are specified here https://grafana.com/docs/tempo/latest/configuration/parquet/
      version: null
    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage
    backend: local
  # Settings for the Admin client storage backend and buckets. Only valid is enterprise.enabled is true.
  admin:
    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/enterprise-traces/latest/config/reference/#admin_client_config
    backend: filesystem

# memcached is for all of the Tempo pieces to coordinate with each other.
# you can use your self memcacherd by set enable: false and host + service
memcached:
  # -- Specified whether the memcached cachce should be enabled
  enabled: true

memcachedExporter:
  # -- Specifies whether the Memcached Exporter should be enabled
  enabled: false


metaMonitoring:
  # ServiceMonitor configuration
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: true
  # metaMonitoringAgent configures the built in Grafana Agent that can scrape metrics and logs and send them to a local or remote destination
  grafanaAgent:
    # -- Controls whether to create PodLogs, MetricsInstance, LogsInstance, and GrafanaAgent CRs to scrape the
    # ServiceMonitors of the chart and ship metrics and logs to the remote endpoints below.
    # Note that you need to configure serviceMonitor in order to have some metrics available.
    enabled: true
    installOperator: true
    logs:
      remote:
        url: ""
    metrics:
      remote:
        url: "http://prometheus-stack-prometheus.default.svc.cluster.local:9090/api/v1/write"
      scrapeK8s:
        enabled: false
        kubeStateMetrics:
          labelSelectors:
            app.kubernetes.io/name: kube-state-metrics
          namespace: default

# Rules for the Prometheus Operator
prometheusRule:
  # -- If enabled, a PrometheusRule resource for Prometheus Operator is created
  enabled: true

minio:
  enabled: true
  mode: standalone
  rootUser: grafana-tempo
  rootPassword: supersecret
  buckets:
    # Default Tempo storage bucket.
    - name: tempo-traces
      policy: none
      purge: false
  persistence:
    size: 5Gi
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
  # Changed the mc config path to '/tmp' from '/etc' as '/etc' is only writable by root and OpenShift will not permit this.
  configPathmc: '/tmp/minio/mc/'

# Configuration for the gateway
gateway:
  # -- Specifies whether the gateway should be enabled
  enabled: ture
  # -- Number of replicas for the gateway
  replicas: 1

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    behavior: {}
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage:
  # -- Enable logging of 2xx and 3xx HTTP requests
  verboseLogging: true

  # Gateway service configuration
  service:
    # -- Port of the gateway service
    port: 80
    # -- Type of the gateway service
    type: ClusterIP
    annotations: {}

  # Gateway ingress configuration
  ingress:
    # -- Specifies whether an ingress for the gateway should be created
    enabled: false
  # Basic auth configuration
  basicAuth:
    # -- Enables basic authentication for the gateway
    enabled: false
    # -- The basic auth username for the gateway
    username: null
    # -- The basic auth password for the gateway
    password: null
    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.
    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes
    # high CPU load.
    htpasswd: >-
      {{ htpasswd (required "'gateway.basicAuth.username' is required" .Values.gateway.basicAuth.username) (required "'gateway.basicAuth.password' is required" .Values.gateway.basicAuth.password) }}
    # -- Existing basic auth secret to use. Must contain '.htpasswd'
    existingSecret: null